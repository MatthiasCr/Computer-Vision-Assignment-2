{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248adb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fiftyone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba34da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7105f7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "from fiftyone.utils.huggingface import load_from_hub\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torchvision.transforms.v2 as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import wandb\n",
    "\n",
    "from src import datasets\n",
    "from src import models\n",
    "from src import training\n",
    "from src import visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287006f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5751b6",
   "metadata": {},
   "source": [
    "## Load Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4e1080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters will be the same for all experiments to make them comparable\n",
    "IMG_SIZE = 64\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 30\n",
    "\n",
    "START_LR = 1e-3\n",
    "END_LR = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38ea171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load fiftyone dataset from huggingface\n",
    "dataset = load_from_hub(\n",
    "    \"MatthiasCr/multimodal-shapes-subset\", \n",
    "    name=\"multimodal-shapes-subset\",\n",
    "    # fewer workers and greater batch size to hopefully avoid getting rate limited\n",
    "    num_workers=2,\n",
    "    batch_size=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc0f22e",
   "metadata": {},
   "source": [
    "Now I convert this fiftyone dataset to torch datasets using the already existing tags for the train / val split. I also create dataloaders for train and valid, as well as a separate dataloader to use for predictions on the valid dataset. For all dataloaders with shuffle=True I specify a generator with fixed seed to make the shuffling deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4920a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_transforms = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.ToImage(),\n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MultimodalDataset(dataset, \"train\", img_transforms)\n",
    "val_dataset = datasets.MultimodalDataset(dataset, \"val\", img_transforms)\n",
    "\n",
    "# use generator with fixed seed for reproducible shuffling\n",
    "generator = torch.Generator()\n",
    "generator.manual_seed(51)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, generator=generator)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n",
    "\n",
    "# loader to conduct sample predictions\n",
    "log_loader = DataLoader(val_dataset, batch_size=5, shuffle=True, num_workers=0, generator=generator)\n",
    "\n",
    "steps_per_epoch = len(train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c016ee",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f3798b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_experiment(model, best_model, fusion_type, device, output_name):\n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    embedding_size = model.get_embedding_size()\n",
    "\n",
    "    optim = Adam(model.parameters(), lr=START_LR)\n",
    "    scheduler = CosineAnnealingLR(optim, T_max=EPOCHS * steps_per_epoch, eta_min=END_LR)\n",
    "    loss_func = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # init wandb run and log config hyperparameters\n",
    "    run = training.initWandbRun(\n",
    "        fusion_type, embedding_size, EPOCHS, BATCH_SIZE, num_params, \"Adam\", \"Cosine Annealing\", START_LR, END_LR\n",
    "    )\n",
    "\n",
    "    # train and log loss\n",
    "    train_loss, val_loss = training.train_model(\n",
    "        model, optim, loss_func, EPOCHS, train_dataloader, val_dataloader, device, run, scheduler=scheduler, output_name=output_name\n",
    "    )\n",
    "\n",
    "    # load best model\n",
    "    model_save_path = f\"../checkpoints/{output_name}.pt\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    best_model.load_state_dict(torch.load(model_save_path, map_location=device))\n",
    "    best_model = best_model.to(device)\n",
    "\n",
    "    # predict on 5 batches of each 5 samples = 25 preditions. Log predictions to wandb\n",
    "    training.log_predictions(best_model, log_loader, device, run, num_batches=5)\n",
    "    \n",
    "    run.finish()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55399dd6",
   "metadata": {},
   "source": [
    "### Late Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc4ecc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "late_model = models.LateFusionNet().to(device)\n",
    "late_model_best = models.LateFusionNet().to(device)\n",
    "late_train_loss, late_val_loss = log_experiment(late_model, late_model_best, \"late\", device, output_name=\"task3_late\")\n",
    "\n",
    "visualization.plot_loss(EPOCHS,\n",
    "    {\n",
    "        \"Late train Loss\": late_train_loss,\n",
    "        \"Late Val Loss\": late_val_loss\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18060774",
   "metadata": {},
   "source": [
    "### Intermediate Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd71200",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_model = models.IntermediateFusionNet(fusion_type=\"cat\").to(device)\n",
    "cat_model_best = models.IntermediateFusionNet(fusion_type=\"cat\").to(device)\n",
    "cat_train_loss, cat_val_loss = log_experiment(cat_model, cat_model_best, \"intermediate (concatenation)\", device, output_name=\"task3_cat\")\n",
    "\n",
    "add_model = models.IntermediateFusionNet(fusion_type=\"add\").to(device)\n",
    "add_model_best = models.IntermediateFusionNet(fusion_type=\"add\").to(device)\n",
    "add_train_loss, add_val_loss = log_experiment(add_model, add_model_best, \"intermediate (addition)\", device, output_name=\"task3_add\")\n",
    "\n",
    "had_model = models.IntermediateFusionNet(fusion_type=\"had\").to(device)\n",
    "had_train_loss, had_val_loss = log_experiment(had_model, \"intermediate (hadamard)\", device, output_name=\"task3_had\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fec08c3",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8803d8",
   "metadata": {},
   "source": [
    "We can now compare how these models performed on the validation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd3c118",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.plot_loss(EPOCHS,\n",
    "    {\n",
    "        \"Concat Valid Loss\": cat_val_loss,\n",
    "        \"Addition Valid Loss\": add_val_loss,\n",
    "        \"Hadamard Valid Loss\": had_val_loss,\n",
    "        \"Late Valid Loss\": late_val_loss\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3744c160",
   "metadata": {},
   "source": [
    "We can also visualize and analyze the experiments in WandB:\n",
    "\n",
    "![](../results/wandb-t3-graphs.png)\n",
    "![](../results/wandb-t3-table.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CompVision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
