{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5124b3f0",
   "metadata": {},
   "source": [
    "# Fusion Architecture Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec905492",
   "metadata": {},
   "source": [
    "In this notebook I will compare late fusion with three variants of intermediate fusion by training four models in total and comparing their perfomance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbc67f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Colab-only setup\n",
    "if \"google.colab\" in sys.modules:\n",
    "    print(\"Running in Google Colab. Setting up repo\")\n",
    "\n",
    "    !git clone https://github.com/MatthiasCr/Computer-Vision-Assignment-2.git\n",
    "    %cd Computer-Vision-Assignment-2\n",
    "    !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba34da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert wandb token\n",
    "!wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7105f7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "from fiftyone.utils.huggingface import load_from_hub\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torchvision.transforms.v2 as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import wandb\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path(\"..\").resolve()\n",
    "sys.path.append(str(project_root))\n",
    "from src import datasets\n",
    "from src import training\n",
    "from src import visualization\n",
    "from src import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287006f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5751b6",
   "metadata": {},
   "source": [
    "## Load Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c68e0cf",
   "metadata": {},
   "source": [
    "As explained in the last notebook I use a subset of the data that i uploaded on huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4e1080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters will be the same for all experiments to make them comparable\n",
    "IMG_SIZE = 64\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "\n",
    "# Initial and target value for learning rate scheduler\n",
    "START_LR = 1e-3\n",
    "END_LR = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38ea171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load fiftyone dataset from huggingface\n",
    "dataset = load_from_hub(\n",
    "    \"MatthiasCr/multimodal-shapes-subset\", \n",
    "    name=\"multimodal-shapes-subset\",\n",
    "    # fewer workers and greater batch size to hopefully avoid getting rate limited\n",
    "    num_workers=2,\n",
    "    batch_size=1000,\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc0f22e",
   "metadata": {},
   "source": [
    "Now I convert this fiftyone dataset to torch datasets using the already existing tags for the train / val split. For this I have defined a class `MultimodalDataset`. I also create dataloaders for train and valid, as well as a separate dataloader (`log_loader`) that I will use for predictions on the valid dataset. For all dataloaders with shuffle = True I specify a generator with fixed seed to make the shuffling deterministic for reproducible results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4920a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_transforms = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.ToImage(),\n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MultimodalDataset(dataset, \"train\", img_transforms)\n",
    "val_dataset = datasets.MultimodalDataset(dataset, \"val\", img_transforms)\n",
    "\n",
    "# use generator with fixed seed for reproducible shuffling\n",
    "generator = torch.Generator()\n",
    "generator.manual_seed(51)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, generator=generator)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n",
    "\n",
    "# loader to conduct sample predictions\n",
    "log_loader = DataLoader(val_dataset, batch_size=5, shuffle=True, num_workers=0, generator=generator)\n",
    "\n",
    "# number of train batches, needed for learning rate scheduling\n",
    "steps_per_epoch = len(train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c016ee",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bcef6a",
   "metadata": {},
   "source": [
    "The `MultimodalDataset` returns the rbg image data, lidar xyza data, and the class (0 = cube, 1 = sphere) index for each item. \n",
    "\n",
    "I have defined a very generalized model training function (`training.train_model()`) that I will use for all training loops across all notebooks. This training function needs a function on how to apply the model on a batch (forward pass). So lets define this function for this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a19990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that tells the training process how to apply the model on a batch of the dataset\n",
    "def apply_model(model, batch):\n",
    "    target = batch[2].to(device)\n",
    "    inputs_rgb = batch[0].to(device)\n",
    "    inputs_xyz = batch[1].to(device)\n",
    "    outputs = model(inputs_rgb, inputs_xyz)\n",
    "    return outputs, target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83124e47",
   "metadata": {},
   "source": [
    "Now i define a function to do a complete experiment cycle for a given model. Every experiment will be logged on Wandb so every experiment starts with an initialization of a new run. Here we pass relevant metrics and hyperparameters as confic, such as the number of parameters, optimizer type, and learning rate scheduler. Then we do the training loop. Inside this training loop we log training, validation loss, and validation accurarcy every epoch. We also checkpoint the training by saving the weights with best validation loss. After the training we load the best checkpoint and do some predictions on 5 validation samples. For each we log the RGB image, the lidar xyza data projected to an image, the label, the prediction, and the prediction probability to wandb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f3798b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_experiment(model, best_model, fusion_type, device, output_name):\n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    optim = Adam(model.parameters(), lr=START_LR)\n",
    "    scheduler = CosineAnnealingLR(optim, T_max=EPOCHS * steps_per_epoch, eta_min=END_LR)\n",
    "    loss_func = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # init wandb run and log config hyperparameters\n",
    "    run = training.initWandbRun(\n",
    "        fusion_type, EPOCHS, BATCH_SIZE, num_params, \"Adam\", \"Cosine Annealing\", START_LR, END_LR\n",
    "    )\n",
    "\n",
    "    # train and log loss\n",
    "    train_loss, val_loss = training.train_model(\n",
    "        model, optim, apply_model, loss_func, EPOCHS, train_dataloader, val_dataloader, device, run, scheduler=scheduler, output_name=output_name\n",
    "    )\n",
    "\n",
    "    # load best model\n",
    "    model_save_path = f\"../checkpoints/{output_name}.pt\"\n",
    "    best_model.load_state_dict(torch.load(model_save_path, map_location=device))\n",
    "    best_model = best_model.to(device)\n",
    "\n",
    "    # predict on 1 batch of 5 samples. Log predictions to wandb\n",
    "    training.log_predictions(best_model, log_loader, device, run, num_batches=1)\n",
    "    \n",
    "    run.finish()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55399dd6",
   "metadata": {},
   "source": [
    "### Late Fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778a8a57",
   "metadata": {},
   "source": [
    "Lets run the experiment for the different fusion models starting with late-fusion.\n",
    "The `LateFusionNet` has an own `LateEmbedder` for both rgb and lidar. The late embedder create full and flattened embeddings with 100 dimensions for each modality. These embeddings are then concatenated and passed into a linear head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc4ecc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "late_model = models.LateFusionNet().to(device)\n",
    "late_model_best = models.LateFusionNet().to(device)\n",
    "late_train_loss, late_val_loss = log_experiment(late_model, late_model_best, \"late\", device, output_name=\"late\")\n",
    "\n",
    "visualization.plot_loss(EPOCHS,\n",
    "    {\n",
    "        \"Late Train Loss\": late_train_loss,\n",
    "        \"Late Val Loss\": late_val_loss\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18060774",
   "metadata": {},
   "source": [
    "### Intermediate Fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb89d3b",
   "metadata": {},
   "source": [
    "For intermediate fusion I have defined a model `IntermediateFusionNet` that has one `IntermediateEmbedder` per modality. These embedders do not produce full embeddings but return intermediate feature maps already after the second convolution. These feature maps are then combined in a way that depends on the type (concatenation, element-wise addition, element-wise multiplication (hadamard product)). After the fusion there is another shared convolutional layer and a linear classification head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd71200",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_model = models.IntermediateFusionNet(fusion_type=\"cat\").to(device)\n",
    "cat_model_best = models.IntermediateFusionNet(fusion_type=\"cat\").to(device)\n",
    "cat_train_loss, cat_val_loss = log_experiment(cat_model, cat_model_best, \"intermediate (concatenation)\", device, output_name=\"cat\")\n",
    "\n",
    "add_model = models.IntermediateFusionNet(fusion_type=\"add\").to(device)\n",
    "add_model_best = models.IntermediateFusionNet(fusion_type=\"add\").to(device)\n",
    "add_train_loss, add_val_loss = log_experiment(add_model, add_model_best, \"intermediate (addition)\", device, output_name=\"add\")\n",
    "\n",
    "had_model = models.IntermediateFusionNet(fusion_type=\"had\").to(device)\n",
    "had_model_best = models.IntermediateFusionNet(fusion_type=\"had\").to(device)\n",
    "had_train_loss, had_val_loss = log_experiment(had_model, had_model_best, \"intermediate (hadamard)\", device, output_name=\"had\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fec08c3",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8803d8",
   "metadata": {},
   "source": [
    "We can now compare how these models performed on the validation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd3c118",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.plot_loss(EPOCHS,\n",
    "    {\n",
    "        \"Concat Valid Loss\": cat_val_loss,\n",
    "        \"Addition Valid Loss\": add_val_loss,\n",
    "        \"Hadamard Valid Loss\": had_val_loss,\n",
    "        \"Late Valid Loss\": late_val_loss\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3744c160",
   "metadata": {},
   "source": [
    "We can also visualize and analyze the experiments in Wandb. The following screenshots show some visualization in the Wandb dashbord. Most importantly we can visualize the loss and accuracy curves and determine which model performed best. We can also inspect the sample predictions to get a very brief feeling on how confident the model predictions are. \n",
    "\n",
    "<img src=\"../results/wandb-t3-graphs.png\">\n",
    "<img src=\"../results/wandb-t3-table.png\">\n",
    "\n",
    "<div style=\"display:grid; grid-template-columns: 1fr 1fr; gap:30px;\">\n",
    "  <img src=\"../results/wandb-t3-valid-loss.png\">\n",
    "  <img src=\"../results/wandb-t3-predictions.png\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a60bb5",
   "metadata": {},
   "source": [
    "The following table summarizes the performance of the 4 models. Validation loss and accuracy are measured from the **best checkpoint**. Train time includes the entire train loop for all 20 epochs and includes the validation phases. GPU memory is measured by wandb by default. \n",
    "\n",
    "| Metric | Late Fusion | Intermediate (Cat) | Intermediate (Add) | Intermediate (Had) |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| Valid Loss | 0.039 | 0.032 | 0.119 | 0.070 |\n",
    "| Valid Accuracy | 0.987 | 0.992 | 0.971 | 0.984 |\n",
    "| Parameter Count | 1,415,051 | 914,201 | 824,201 | 824,201 |\n",
    "| Train Time (s) | 22.17 | 25.71 | 24.38 | 24.76 |\n",
    "| GPU Memory (MB) | 601 | 773 | 775 | 777 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361cfae2",
   "metadata": {},
   "source": [
    "The best performing model was **Intermediate Fusion with Concatenation**. Its best checkpoint has a validation loss of 0.032 and a validation accuracy of 0.992. But also pretty close comes late fusion and intermediate with hadamard. Intermediate with addition has lowest accuracy and highest loss. \n",
    "\n",
    "It is interesting that the late fusion model has around 50% more parameters than the intermediate concat model, but the performance of the intermediate concat model is pretty competitive and even slightly better. Also the intermediate hadamard model performs quite well given it has even less parameters. This shows that intermediate fusion is a very fitting architecture for this problem. The idea of intermediate fusion is to combine information from both modalities somewhere during the creation of one joint representation, rather than creating two fully separate embeddings. This way intermediate fusion allows for more joint, cross-modal feature interactions before making a decision. Concatenation is specific preserves the full information from both modalities and letting the model learn itself how to combine them inside the next convolutional layer. This gives the model a little bit more flexibility rather than forcing to combine the modalities with addition or multipliction.\n",
    "\n",
    "A possible hypothesis for the comparatively bad perfomance of the intermediate addition model is that addition indiscriminately blends features from both modalities, averaging their activations and thereby weakening **modality-specific** signals. If, for example, one modality is noisy or less informative its features can directly corrupt the other modality's representation. In contrast, the hadamard product features are amplified when **both modalities** produce strong activations and suppressed where one of the modalites is rather weak. This encourages agreement between the modalities and reduces the influence of noisy or irrelevant features, which can lead to more meaningful joint representations and better performance simple addition."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CompVision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
