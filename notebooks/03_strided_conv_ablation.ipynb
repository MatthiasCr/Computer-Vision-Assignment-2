{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20abf75b",
   "metadata": {},
   "source": [
    "# MaxPool2d to Strided Convolution Ablation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b44e56a",
   "metadata": {},
   "source": [
    "In this notebook I will retrain the four models from the previous notebooks but use a different downsampling method. I specific I will replace `MaxPool2D` with strided convolutions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ba83f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Colab-only setup\n",
    "if \"google.colab\" in sys.modules:\n",
    "    print(\"Running in Google Colab. Setting up repo\")\n",
    "\n",
    "    !git clone https://github.com/MatthiasCr/Computer-Vision-Assignment-2.git\n",
    "    %cd Computer-Vision-Assignment-2\n",
    "    !pip install -r requirements.txt\n",
    "    %cd notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816f1b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert wandb token\n",
    "!wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844f3a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "from fiftyone.utils.huggingface import load_from_hub\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torchvision.transforms.v2 as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import wandb\n",
    "\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path(\"..\").resolve()\n",
    "sys.path.append(str(project_root))\n",
    "from src import datasets\n",
    "from src import training\n",
    "from src import visualization\n",
    "from src import models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662913d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70631a2",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebde1c78",
   "metadata": {},
   "source": [
    "As in the previous notebooks I start by loading the data from huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0e1630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters will be the same for all experiments to make them comparable\n",
    "IMG_SIZE = 64\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "\n",
    "START_LR = 1e-3\n",
    "END_LR = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c8e601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load fiftyone dataset from huggingface\n",
    "dataset = load_from_hub(\n",
    "    \"MatthiasCr/multimodal-shapes-subset\", \n",
    "    name=\"multimodal-shapes-subset\",\n",
    "    # fewer workers and greater batch size to hopefully avoid getting rate limited\n",
    "    num_workers=2,\n",
    "    batch_size=500,\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a78e52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_transforms = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.ToImage(),\n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MultimodalDataset(dataset, \"train\", img_transforms)\n",
    "val_dataset = datasets.MultimodalDataset(dataset, \"val\", img_transforms)\n",
    "\n",
    "# use generator with fixed seed for reproducible shuffling\n",
    "generator = torch.Generator()\n",
    "generator.manual_seed(51)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, generator=generator)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n",
    "\n",
    "# loader to conduct sample predictions\n",
    "log_loader = DataLoader(val_dataset, batch_size=5, shuffle=True, num_workers=0, generator=generator)\n",
    "\n",
    "# number of train batches, needed for learning rate scheduling\n",
    "steps_per_epoch = len(train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d570b9cd",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f231c3ed",
   "metadata": {},
   "source": [
    "I use the same hyperparameters and the same experiment function as in the previous notebook. This way we can directly compare the runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04638ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that tells the training process how to apply the model on a batch of the dataset\n",
    "def apply_model(model, batch):\n",
    "    target = batch[2].to(device)\n",
    "    inputs_rgb = batch[0].to(device)\n",
    "    inputs_xyz = batch[1].to(device)\n",
    "    outputs = model(inputs_rgb, inputs_xyz)\n",
    "    return outputs, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698951bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_experiment(model, best_model, fusion_type, device, output_name):\n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    optim = Adam(model.parameters(), lr=START_LR)\n",
    "    scheduler = CosineAnnealingLR(optim, T_max=EPOCHS * steps_per_epoch, eta_min=END_LR)\n",
    "    loss_func = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # init wandb run and log config hyperparameters\n",
    "    run = training.initWandbRun(\n",
    "        fusion_type, EPOCHS, BATCH_SIZE, num_params, \"Adam\", \"Cosine Annealing\", START_LR, END_LR\n",
    "    )\n",
    "\n",
    "    # train and log loss\n",
    "    train_loss, val_loss = training.train_model(\n",
    "        model, optim, apply_model, loss_func, EPOCHS, train_dataloader, val_dataloader, device, run, scheduler=scheduler, output_name=output_name\n",
    "    )\n",
    "\n",
    "    # load best model\n",
    "    model_save_path = f\"../checkpoints/{output_name}.pt\"\n",
    "    best_model.load_state_dict(torch.load(model_save_path, map_location=device))\n",
    "    best_model = best_model.to(device)\n",
    "\n",
    "    # predict on 4 batches of each 5 samples = 20 preditions. Log predictions to wandb\n",
    "    training.log_predictions(best_model, log_loader, device, run, num_batches=4)\n",
    "    \n",
    "    run.finish()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ff4ac4",
   "metadata": {},
   "source": [
    "Now we can start training the four models. I use the same models as in the previous notebook but pass the extra parameter `embedder_type = \"strided\"`. This replaces maxpooling layers with the identity, and changes the stride of all convolutions from 1 to 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef203cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "late_model = models.LateFusionNet(embedder_type=\"strided\").to(device)\n",
    "late_model_best = models.LateFusionNet(embedder_type=\"strided\").to(device)\n",
    "late_train_loss, late_val_loss = log_experiment(late_model, late_model_best, \"late\", device, output_name=\"strided_late\")\n",
    "\n",
    "cat_model = models.IntermediateFusionNet(fusion_type=\"cat\", embedder_type=\"strided\").to(device)\n",
    "cat_model_best = models.IntermediateFusionNet(fusion_type=\"cat\", embedder_type=\"strided\").to(device)\n",
    "cat_train_loss, cat_val_loss = log_experiment(cat_model, cat_model_best, \"intermediate (concatenation)\", device, output_name=\"strided_cat\")\n",
    "\n",
    "add_model = models.IntermediateFusionNet(fusion_type=\"add\", embedder_type=\"strided\").to(device)\n",
    "add_model_best = models.IntermediateFusionNet(fusion_type=\"add\", embedder_type=\"strided\").to(device)\n",
    "add_train_loss, add_val_loss = log_experiment(add_model, add_model_best, \"intermediate (addition)\", device, output_name=\"strided_add\")\n",
    "\n",
    "had_model = models.IntermediateFusionNet(fusion_type=\"had\", embedder_type=\"strided\").to(device)\n",
    "had_model_best = models.IntermediateFusionNet(fusion_type=\"had\", embedder_type=\"strided\").to(device)\n",
    "had_train_loss, had_val_loss = log_experiment(had_model, had_model_best, \"intermediate (hadamard)\", device, output_name=\"strided_had\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4551db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.plot_loss(EPOCHS,\n",
    "    {\n",
    "        \"Concat Valid Loss\": cat_val_loss,\n",
    "        \"Addition Valid Loss\": add_val_loss,\n",
    "        \"Hadamard Valid Loss\": had_val_loss,\n",
    "        \"Late Valid Loss\": late_val_loss\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d811d9",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66732f4",
   "metadata": {},
   "source": [
    "The following screenshots show the direct comparison of the different valid loss curves of the fusion models with strided convolutional downsampling compared to the models with maxpool from the last notebook. \n",
    "\n",
    "<div style=\"display:grid; grid-template-columns: 1fr 1fr; gap:10px;\">\n",
    "  <img src=\"../results/wandb-t4-late-graphs.png\">\n",
    "  <img src=\"../results/wandb-t4-concat-graphs.png\">\n",
    "  <img src=\"../results/wandb-t4-add-graphs.png\">\n",
    "  <img src=\"../results/wandb-t4-had-graphs.png\">\n",
    "</div>\n",
    "\n",
    "We can see that the models with strided convolutions consistently underperform the corresponding maxpool version. The following table shows the exact values of the strided models. In the brackets behind the values are the differences to the corresponding maxpool run.\n",
    "\n",
    "\n",
    "| Metric | Strided Late | Strided Concat | Strided Addition | Strided Hadamard |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| Valid Loss | 0.254 (+0.215) | 0.357 (+0.325) | 0.395 (+0.276) | 0.179 (+0.109) |\n",
    "| Valid Accuracy | 0.929 (-0.058) | 0.841 (-0.151) | 0.820 (-0.151) | 0.971 (-0.013) |\n",
    "| Parameter Count | 1,415,051 (=) | 914,201 (=) | 824,201 (=) | 824,201 (=) |\n",
    "| Train Time (s) | 16.15 (-6.02) | 15.25 (-9.92) | 14.97 (-9.41) | 14.82 (-9.94) |\n",
    "| GPU Memory (MB) | 779 (+178) | 779 (+6) | 779 (+4) | 779 (+2) |\n",
    "\n",
    "As we can see in the table, the strided models performed worse than the corresponding maxpool versions for every fusion type. All validation losses increased strongly while the validation accuracies decreased. Especially the intermediate fusion with concatenation and addition are negatively affected having accuracies of only 82% - 84%. On the other hand did the training times of the strided models decrease by roughly around 33% - 66% compared with the maxpool models. The recorded allocated GPU memory is exactly the same for all models, this could be either due to the fact that all runs were executed subsequently with no time in between, or to unknown google colab restrictions, or it could have something to do with how wandb logs these metrics internally. \n",
    "\n",
    "All strided models have exactly the same number of parameters as the maxpool versions. The number of parameters only depends on the number of filters and filter sizes in the convolutional layers, and the dimensions of the linear layers which did not change. Maxpooling itself has no learnable parameters. However when the stride of the convolutions is changed from 1 to 2 then the output feature maps are smaller. While this is the indended downsampling effect, it is noteworthy that this also reduces the models complexity and flexibility. The convolution and the downsampling is done **in one step** whereas the maxpool architectures decouple these two operations. With MaxPool2d, the convolution layers first compute dense, high-resolution feature maps, and the pooling operation then performs a simple selection that preserves the strongest activations. This separation allows the network to learn richer local features before reducing resolution. In contrast, using stride-2 convolutions forces the network to discard spatial information already during feature extraction. Each filter observes fewer spatial positions, and fine-grained details may be skipped entirely. This is particularly harmful for intermediate fusion, where precise alignment between RGB and Lidar features is important. The loss of dense details reduces the effectiveness of cross-modal interactions, which likely explains the strong degradation for concatenation and addition fusion.\n",
    "\n",
    "On the other side this is also the reason that the training times decreased so drastically. The strided convolutions require much less FLOPs and therefore also less time and energy. My recommendation would be to consider replacing maxpool with strided convolutions only when computational efficiency, faster training, or deployment constraints are very important. If you want to get peak accuracy performance then, based on my experiments, MaxPooling would be the better choice."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CompVision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
