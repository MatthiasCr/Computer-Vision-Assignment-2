{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8745a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fiftyone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4570b3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d411c1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "from fiftyone.utils.huggingface import load_from_hub\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torchvision.transforms.v2 as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import wandb\n",
    "\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path(\"..\").resolve()\n",
    "sys.path.append(str(project_root))\n",
    "from src import datasets\n",
    "from src import training\n",
    "from src import visualization\n",
    "from src import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2562913b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1da764",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebdf26f",
   "metadata": {},
   "source": [
    "As we did in the previous notebooks we start by loading the data as a fiftyone dataset from huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c8ac46",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 64\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1aaf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load fiftyone dataset from huggingface\n",
    "dataset = load_from_hub(\n",
    "    \"MatthiasCr/multimodal-shapes-subset\", \n",
    "    name=\"multimodal-shapes-subset\",\n",
    "    # fewer workers and greater batch size to hopefully avoid getting rate limited\n",
    "    num_workers=2,\n",
    "    batch_size=1000,\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb60ee9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_transforms = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.ToImage(),\n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MultimodalDataset(dataset, \"train\", img_transforms)\n",
    "val_dataset = datasets.MultimodalDataset(dataset, \"val\", img_transforms)\n",
    "\n",
    "# use generator with fixed seed for reproducible shuffling\n",
    "generator = torch.Generator()\n",
    "generator.manual_seed(51)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, generator=generator)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n",
    "\n",
    "# number of train batches, needed for learning rate scheduling\n",
    "steps_per_epoch = len(train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310e87b6",
   "metadata": {},
   "source": [
    "## Baseline Lidar Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a590b8cb",
   "metadata": {},
   "source": [
    "We first train a single modal classifier that only uses lidar data to do the cube/sphere classification. This `LidarClassifier` consists of an embedder and a classification head. The purpose of this model is to define an embedding space for lidar data and to have a classifier head that works on these embeddings. We will later project image embeddings onto this lidar embedding space so that we can use this classifier head (without the embedder) for classification of image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715d25c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lidar_classifier = models.LidarClassifier(emb_size=200, normalize_embs=True).to(device)\n",
    "lidar_classifier_num_params = sum(p.numel() for p in lidar_classifier.parameters())\n",
    "epochs = 20\n",
    "start_lr = 1e-4\n",
    "end_lr = 1e-6\n",
    "\n",
    "optim = Adam(lidar_classifier.parameters(), lr=start_lr)\n",
    "scheduler = CosineAnnealingLR(optim, T_max=epochs * steps_per_epoch, eta_min=end_lr)\n",
    "loss_func = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d713e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_classifier_model(model, batch):\n",
    "    # only lidar data used\n",
    "    _, inputs_xyz, target = batch\n",
    "    inputs_xyz = inputs_xyz.to(device)\n",
    "    target = target.to(device)\n",
    "    outputs = model(raw_data=inputs_xyz)\n",
    "    return outputs, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568d7946",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = training.initWandbRun(\n",
    "    \"\", epochs, BATCH_SIZE, lidar_classifier_num_params, \"Adam\", \"Cosine Annealing\", start_lr, end_lr\n",
    ")\n",
    "\n",
    "classifier_train_loss, classifier_val_loss = training.train_model(\n",
    "    lidar_classifier, \n",
    "    optim, \n",
    "    apply_classifier_model, \n",
    "    loss_func, \n",
    "    epochs, \n",
    "    train_dataloader, \n",
    "    val_dataloader, \n",
    "    device, \n",
    "    run, \n",
    "    scheduler=scheduler, \n",
    "    output_name=\"lidar_classifier\"\n",
    ")\n",
    "\n",
    "run.finish()\n",
    "\n",
    "visualization.plot_loss(epochs,\n",
    "    {\n",
    "        \"Classifier Train Loss\": classifier_train_loss,\n",
    "        \"Classifier Val Loss\": classifier_val_loss\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bd6ee4",
   "metadata": {},
   "source": [
    "Now load the best checkpoint and freeze the model so we can use it later without altering it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43033216",
   "metadata": {},
   "outputs": [],
   "source": [
    "lidar_classifier = models.LidarClassifier()\n",
    "lidar_classifier.load_state_dict(\n",
    "    torch.load(\"../checkpoints/lidar_classifier.pt\", map_location=device))\n",
    "lidar_classifier = lidar_classifier.to(device)\n",
    "\n",
    "for param in lidar_classifier.parameters():\n",
    "    param.requires_grad = False\n",
    "lidar_classifier.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921d54f8",
   "metadata": {},
   "source": [
    "## CILP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a83cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cilp_model = models.ContrastivePretraining(batch_size=BATCH_SIZE).to(device)\n",
    "cilp_num_params = sum(p.numel() for p in cilp_model.parameters())\n",
    "epochs = 40\n",
    "lr = 1e-2\n",
    "\n",
    "optim = Adam(cilp_model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1793f4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_cilp_model(model, batch):\n",
    "    # cilp model doesn't use the class information\n",
    "    inputs_rgb, inputs_xyz, _ = batch\n",
    "    inputs_rgb = inputs_rgb.to(device)\n",
    "    inputs_xyz = inputs_xyz.to(device)\n",
    "    logits_per_img, logits_per_lidar = model(inputs_rgb, inputs_xyz)\n",
    "    return logits_per_img, logits_per_lidar\n",
    "\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_lidar = nn.CrossEntropyLoss()\n",
    "ground_truth = torch.arange(BATCH_SIZE, dtype=torch.long).to(device)\n",
    "\n",
    "def cilp_loss(logits_per_img, logits_per_lidar):\n",
    "    return (loss_img(logits_per_img, ground_truth) + loss_lidar(logits_per_lidar, ground_truth)) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3307b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = training.initWandbRun(\n",
    "    \"\", epochs, BATCH_SIZE, cilp_num_params, \"Adam\", \"\", lr, lr\n",
    ")\n",
    "\n",
    "cilp_train_loss, cilp_val_loss = training.train_model(\n",
    "    cilp_model, \n",
    "    optim,\n",
    "    apply_cilp_model, \n",
    "    cilp_loss, \n",
    "    epochs, \n",
    "    train_dataloader, \n",
    "    val_dataloader, \n",
    "    device, \n",
    "    run, \n",
    "    scheduler=None, \n",
    "    output_name=\"cilp\", \n",
    "    calc_accuracy=False\n",
    ")\n",
    "\n",
    "visualization.plot_loss(epochs,\n",
    "    {\n",
    "        \"Cilp Train Loss\": cilp_train_loss,\n",
    "        \"Cilp Val Loss\": cilp_val_loss\n",
    "    }\n",
    ")\n",
    "\n",
    "visualization.plot_similarity_matrix(cilp_model, apply_cilp_model, val_dataloader, device, run)\n",
    "\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ebd395",
   "metadata": {},
   "source": [
    "Now load the best cilp checkpoint and freeze the model for further usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd19fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "cilp_model = models.ContrastivePretraining(batch_size=BATCH_SIZE)\n",
    "cilp_model.load_state_dict(torch.load(\"../checkpoints/cilp.pt\", map_location=device))\n",
    "cilp_model = cilp_model.to(device)\n",
    "\n",
    "for param in cilp_model.parameters():\n",
    "    param.requires_grad = False\n",
    "cilp_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dfc0aa",
   "metadata": {},
   "source": [
    "## Projector Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc8680d",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_embedding_dim = cilp_model.get_embedding_size()\n",
    "target_embedding_dim = lidar_classifier.get_embedding_size()\n",
    "\n",
    "projector = models.Projector(source_embedding_dim, target_embedding_dim).to(device)\n",
    "projector_num_params = sum(p.numel() for p in projector.parameters())\n",
    "epochs = 40\n",
    "lr = 1e-4\n",
    "optim = torch.optim.Adam(projector.parameters(), lr=lr, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af1764d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_projector_model(model, batch):\n",
    "    rgb_img, lidar_xyz, _ = batch\n",
    "    rgb_img = rgb_img.to(device)\n",
    "    lidar_xyz = lidar_xyz.to(device)\n",
    "    imb_embs = cilp_model.img_embedder(rgb_img)\n",
    "    lidar_embs = lidar_classifier.get_embs(lidar_xyz)\n",
    "    pred_lidar_embs = model(imb_embs)\n",
    "    return pred_lidar_embs, lidar_embs\n",
    "\n",
    "def projector_loss(pred_lidar_embs, lidar_embs):\n",
    "    return nn.MSELoss()(pred_lidar_embs, lidar_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e72559",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = training.initWandbRun(\n",
    "    \"\", epochs, BATCH_SIZE, projector_num_params, \"Adam\", \"\", lr, lr\n",
    ")\n",
    "\n",
    "projector_train_loss, projector_val_loss = training.train_model(\n",
    "    projector, \n",
    "    optim,\n",
    "    apply_projector_model, \n",
    "    projector_loss, \n",
    "    epochs, \n",
    "    train_dataloader, \n",
    "    val_dataloader, \n",
    "    device, \n",
    "    run, \n",
    "    scheduler=None, \n",
    "    output_name=\"projector\", \n",
    "    calc_accuracy=False\n",
    ")\n",
    "\n",
    "run.finish()\n",
    "\n",
    "visualization.plot_loss(epochs,\n",
    "    {\n",
    "        \"Projector Train Loss\": projector_train_loss,\n",
    "        \"Projector Val Loss\": projector_val_loss\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b2e9e5",
   "metadata": {},
   "source": [
    "Again, load the best saved weights for the projector model. This time we don't freeze it as we fine tune it inside the final RGB-to-Lidar classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fba379",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = models.Projector(source_embedding_dim, target_embedding_dim)\n",
    "projector.load_state_dict(torch.load(\"../checkpoints/projector.pt\", map_location=device))\n",
    "projector = projector.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0335fb69",
   "metadata": {},
   "source": [
    "## Final RGB-to-Lidar Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72489efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgbToLiderClassifier = models.RGB2LiDARClassifier(projector, cilp_model, lidar_classifier)\n",
    "rgbToLidar_num_params = sum(p.numel() for p in rgbToLiderClassifier.parameters())\n",
    "\n",
    "epochs = 30\n",
    "start_lr = 1e-2\n",
    "end_lr = 1e-6\n",
    "optim = torch.optim.Adam(projector.parameters(), lr=lr)\n",
    "scheduler = CosineAnnealingLR(optim, T_max=epochs * steps_per_epoch, eta_min=end_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23192af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rgb_Lidar_Classifier_model(model, batch):\n",
    "    inputs_rgb, _, target = batch\n",
    "    inputs_rgb = inputs_rgb.to(device)\n",
    "    target = target.to(device)\n",
    "    outputs = model(inputs_rgb)\n",
    "    return outputs, target\n",
    "\n",
    "loss_func = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0638757a",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = training.initWandbRun(\n",
    "    \"\", epochs, BATCH_SIZE, rgbToLidar_num_params, \"Adam\", \"\", lr, lr\n",
    ")\n",
    "\n",
    "final_train_loss, final_val_loss = training.train_model(\n",
    "    rgbToLiderClassifier, \n",
    "    optim,\n",
    "    apply_rgb_Lidar_Classifier_model, \n",
    "    loss_func, \n",
    "    epochs, \n",
    "    train_dataloader, \n",
    "    val_dataloader, \n",
    "    device, \n",
    "    run, \n",
    "    scheduler=scheduler, \n",
    "    output_name=\"rgb_to_lidar_classifier\"\n",
    ")\n",
    "\n",
    "run.finish()\n",
    "\n",
    "visualization.plot_loss(epochs,\n",
    "    {\n",
    "        \"Rgb-to-Lidar Train Loss\": final_train_loss,\n",
    "        \"Rgb-to-Lidar Val Loss\": final_val_loss\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640c087f",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector.eval()\n",
    "with torch.no_grad():\n",
    "    batch = next(iter(val_dataloader))\n",
    "    rgb, lidar_xyza, labels = batch\n",
    "    rgb = rgb.to(device); lidar_xyza = lidar_xyza.to(device); labels = labels.to(device)\n",
    "\n",
    "    # True LiDAR path\n",
    "    true_embs = lidar_classifier.get_embs(lidar_xyza)\n",
    "    logits_true = lidar_classifier(data_embs=true_embs).squeeze(1)\n",
    "    acc_true = ((torch.sigmoid(logits_true) >= 0.5) == labels.bool()).float().mean()\n",
    "\n",
    "    # Projected path\n",
    "    img_embs = cilp_model.img_embedder(rgb)\n",
    "    proj_embs = projector(img_embs)\n",
    "    logits_proj = lidar_classifier(data_embs=proj_embs).squeeze(1)\n",
    "    acc_proj = ((torch.sigmoid(logits_proj) >= 0.5) == labels.bool()).float().mean()\n",
    "\n",
    "    # Alignment stats\n",
    "    cosine = torch.nn.functional.cosine_similarity(proj_embs, true_embs, dim=1).mean()\n",
    "    norm_true = true_embs.norm(dim=1).mean()\n",
    "    norm_proj = proj_embs.norm(dim=1).mean()\n",
    "\n",
    "print(f\"Acc true: {acc_true:.3f} | Acc proj: {acc_proj:.3f}\")\n",
    "print(f\"Cosine mean: {cosine:.3f} | Norm true: {norm_true:.3f} | Norm proj: {norm_proj:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CompVision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
