{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9448028",
   "metadata": {},
   "source": [
    "# CILP Assessment Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c45fa1",
   "metadata": {},
   "source": [
    "The goal of this notebook is to train a model that projects between two embeddings spaces. Here the projector should turn image embeddings onto lidar embeddings so we can use a lidar classifier to classify images. For this we first train a single modal classifier on only lidar data. This model defines the lidar embedding space. Then we train a contrastive learning model \"CILP\" (Contrastive-Image-Lidar-Pretraining). The image embedder from our CILP model defines the image embedding space. We then train a projector model that takes an image embedding produced by CILP and turns it into a lidar embedding that is as close as possible to the lidar embedding produced by the first lidar model. Lastly we put everything together into an RGB2Lidar classifier that takes images, produces embeddings using CILP, turns these image embeddings onto lidar embeddings, and gives them to the pretrained and frozen lidar classifier head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a61f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Colab-only setup\n",
    "if \"google.colab\" in sys.modules:\n",
    "    print(\"Running in Google Colab. Setting up repo\")\n",
    "\n",
    "    !git clone https://github.com/MatthiasCr/Computer-Vision-Assignment-2.git\n",
    "    %cd Computer-Vision-Assignment-2/notebooks\n",
    "    !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4570b3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert wandb token \n",
    "!wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d411c1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "from fiftyone.utils.huggingface import load_from_hub\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torchvision.transforms.v2 as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import wandb\n",
    "\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path(\"..\").resolve()\n",
    "sys.path.append(str(project_root))\n",
    "from src import datasets\n",
    "from src import training\n",
    "from src import visualization\n",
    "from src import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2562913b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1da764",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebdf26f",
   "metadata": {},
   "source": [
    "As we did in the previous notebooks we start by loading the data as a fiftyone dataset from huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c8ac46",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 64\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1aaf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load fiftyone dataset from huggingface\n",
    "dataset = load_from_hub(\n",
    "    \"MatthiasCr/multimodal-shapes-subset\", \n",
    "    name=\"multimodal-shapes-subset\",\n",
    "    # fewer workers and greater batch size to hopefully avoid getting rate limited\n",
    "    num_workers=2,\n",
    "    batch_size=1000,\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb60ee9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_transforms = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.ToImage(),\n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MultimodalDataset(dataset, \"train\", img_transforms)\n",
    "val_dataset = datasets.MultimodalDataset(dataset, \"val\", img_transforms)\n",
    "\n",
    "# use generator with fixed seed for reproducible shuffling\n",
    "generator = torch.Generator()\n",
    "generator.manual_seed(51)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, generator=generator)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n",
    "\n",
    "# number of train batches, needed for learning rate scheduling\n",
    "steps_per_epoch = len(train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310e87b6",
   "metadata": {},
   "source": [
    "## Baseline Lidar Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a590b8cb",
   "metadata": {},
   "source": [
    "We first train a single modal classifier that only uses lidar data to do the cube/sphere classification. This `LidarClassifier` consists of an embedder and a classification head. The embedder dimension is 200.\n",
    "\n",
    "The purpose of this model is to define an embedding space for lidar data and to have a classifier head that works on these embeddings. We will later project image embeddings onto this lidar embedding space so that we can use this classifier head (without the embedder) for classification of image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715d25c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lidar_classifier = models.LidarClassifier(emb_size=200, normalize_embs=True).to(device)\n",
    "lidar_classifier_num_params = sum(p.numel() for p in lidar_classifier.parameters())\n",
    "epochs = 20\n",
    "start_lr = 1e-4\n",
    "end_lr = 1e-6\n",
    "\n",
    "optim = Adam(lidar_classifier.parameters(), lr=start_lr)\n",
    "scheduler = CosineAnnealingLR(optim, T_max=epochs * steps_per_epoch, eta_min=end_lr)\n",
    "loss_func = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d713e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_classifier_model(model, batch):\n",
    "    # only lidar data used\n",
    "    _, inputs_xyz, target = batch\n",
    "    inputs_xyz = inputs_xyz.to(device)\n",
    "    target = target.to(device)\n",
    "    outputs = model(raw_data=inputs_xyz)\n",
    "    return outputs, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568d7946",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = training.initWandbRun(\n",
    "    \"\", epochs, BATCH_SIZE, lidar_classifier_num_params, \"Adam\", \"Cosine Annealing\", start_lr, end_lr\n",
    ")\n",
    "\n",
    "classifier_train_loss, classifier_val_loss = training.train_model(\n",
    "    lidar_classifier, \n",
    "    optim, \n",
    "    apply_classifier_model, \n",
    "    loss_func, \n",
    "    epochs, \n",
    "    train_dataloader, \n",
    "    val_dataloader, \n",
    "    device, \n",
    "    run, \n",
    "    scheduler=scheduler, \n",
    "    output_name=\"lidar_classifier\"\n",
    ")\n",
    "\n",
    "run.finish()\n",
    "\n",
    "visualization.plot_loss(epochs,\n",
    "    {\n",
    "        \"Classifier Train Loss\": classifier_train_loss,\n",
    "        \"Classifier Val Loss\": classifier_val_loss\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1f8cee",
   "metadata": {},
   "source": [
    "We can visualize the loss and accuracy curves on Wandb. The validation accuracy quickly reaches 0.999.\n",
    "\n",
    "<img src=\"../results/wandb-t5-lidar-graph.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bd6ee4",
   "metadata": {},
   "source": [
    "Now we load the best checkpoint and freeze the model so we can use it later without altering it. The frozen classification head will be metric on how good the projector model can imitate the lidar embeddings from lidar_classifier model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43033216",
   "metadata": {},
   "outputs": [],
   "source": [
    "lidar_classifier = models.LidarClassifier()\n",
    "lidar_classifier.load_state_dict(\n",
    "    torch.load(\"../checkpoints/lidar_classifier.pt\", map_location=device))\n",
    "lidar_classifier = lidar_classifier.to(device)\n",
    "\n",
    "# freezing\n",
    "for param in lidar_classifier.parameters():\n",
    "    param.requires_grad = False\n",
    "lidar_classifier.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921d54f8",
   "metadata": {},
   "source": [
    "## CILP Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46cc323",
   "metadata": {},
   "source": [
    "Now we train the CILP model. The goal is to create a model that creates aligned embeddings for both modalities using self-supervised contrastive learning. The embeddings have 200 dimensions which aligns with embedding size from the lidar classifier. This will make things a little bit easier for the projector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a83cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cilp_model = models.ContrastivePretraining(batch_size=BATCH_SIZE).to(device)\n",
    "cilp_num_params = sum(p.numel() for p in cilp_model.parameters())\n",
    "epochs = 40\n",
    "lr = 1e-2\n",
    "\n",
    "optim = Adam(cilp_model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1793f4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_cilp_model(model, batch):\n",
    "    # cilp model doesn't use the class information\n",
    "    inputs_rgb, inputs_xyz, _ = batch\n",
    "    inputs_rgb = inputs_rgb.to(device)\n",
    "    inputs_xyz = inputs_xyz.to(device)\n",
    "    logits_per_img, logits_per_lidar = model(inputs_rgb, inputs_xyz)\n",
    "    return logits_per_img, logits_per_lidar\n",
    "\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_lidar = nn.CrossEntropyLoss()\n",
    "ground_truth = torch.arange(BATCH_SIZE, dtype=torch.long).to(device)\n",
    "\n",
    "def cilp_loss(logits_per_img, logits_per_lidar):\n",
    "    return (loss_img(logits_per_img, ground_truth) + loss_lidar(logits_per_lidar, ground_truth)) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3307b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = training.initWandbRun(\n",
    "    \"\", epochs, BATCH_SIZE, cilp_num_params, \"Adam\", \"\", lr, lr\n",
    ")\n",
    "\n",
    "cilp_train_loss, cilp_val_loss = training.train_model(\n",
    "    cilp_model, \n",
    "    optim,\n",
    "    apply_cilp_model, \n",
    "    cilp_loss, \n",
    "    epochs, \n",
    "    train_dataloader, \n",
    "    val_dataloader, \n",
    "    device, \n",
    "    run, \n",
    "    scheduler=None, \n",
    "    output_name=\"cilp\", \n",
    "    calc_accuracy=False\n",
    ")\n",
    "\n",
    "visualization.plot_loss(epochs,\n",
    "    {\n",
    "        \"Cilp Train Loss\": cilp_train_loss,\n",
    "        \"Cilp Val Loss\": cilp_val_loss\n",
    "    }\n",
    ")\n",
    "\n",
    "# create and log similarity matrix for first validation batch\n",
    "visualization.plot_similarity_matrix(cilp_model, apply_cilp_model, val_dataloader, device, run)\n",
    "\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e6ac05",
   "metadata": {},
   "source": [
    "The following graphs show again the resulting loss curves. The mininum validation loss we got is **2.59**.\n",
    "\n",
    "<img src=\"../results/wandb-t5-cilp.png\">\n",
    "\n",
    "The similarity matrix is a metric to visualize and evaluate the contrastive learning model. It shows the similarities across all image and lidar embeddings of the first validation batch. The diagonal represents the similarities of the matching image/lidar samples while all other points are negative pairings. We can see that the diagnoal is close to one which indicates that matching image and lidar embeddings are very similar. Most of the other points are close to zero which is also desired. However, there is still much noise which indicates that many negatives are still too similar. Possible ways to further improve this model is to increase the batch size which would give the model more negative examples, or to just train longer. An additional method could be to detect and oversample negatives that are especially hard.\n",
    "\n",
    "This similarity matrix is also logged as an artifact on Wandb.\n",
    "\n",
    "<img src=\"../results/cilp-similarity-matrix.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ebd395",
   "metadata": {},
   "source": [
    "Now load the best cilp checkpoint and freeze the model for further usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd19fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "cilp_model = models.ContrastivePretraining(batch_size=BATCH_SIZE)\n",
    "cilp_model.load_state_dict(torch.load(\"../checkpoints/cilp.pt\", map_location=device))\n",
    "cilp_model = cilp_model.to(device)\n",
    "\n",
    "# freezing\n",
    "for param in cilp_model.parameters():\n",
    "    param.requires_grad = False\n",
    "cilp_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dfc0aa",
   "metadata": {},
   "source": [
    "## Projector Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6770fc6",
   "metadata": {},
   "source": [
    "Now comes the projector model. After a lot of trying around I decided to just use a very simple and relatively small linear model. Since I only use a smaller subset of the complete training data, a more complex model tended to overfit extremly fast. I decided on the following architecture with 40300 parameters that is implemented in the `models.Projector` class. Input and output dimensions are both 200. Also I added a normalization at the end of the projector, since both the image as well as the lidar embeddings are normalized. This could make things easier for the projector.\n",
    "\n",
    "\n",
    "```python\n",
    "nn.Sequential(\n",
    "    nn.Linear(img_emb_size, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, lidar_emb_size)\n",
    ")\n",
    "```\n",
    "\n",
    "Since the model still tended to overfit quite fast I chose a small learning rate, added weight-decay regularization and, as always, checkpointed the model at the best validtion loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc8680d",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_embedding_dim = cilp_model.get_embedding_size()\n",
    "target_embedding_dim = lidar_classifier.get_embedding_size()\n",
    "\n",
    "projector = models.Projector(source_embedding_dim, target_embedding_dim).to(device)\n",
    "projector_num_params = sum(p.numel() for p in projector.parameters())\n",
    "epochs = 40\n",
    "lr = 1e-4\n",
    "optim = torch.optim.Adam(projector.parameters(), lr=lr, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af1764d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_projector_model(model, batch):\n",
    "    rgb_img, lidar_xyz, _ = batch\n",
    "    rgb_img = rgb_img.to(device)\n",
    "    lidar_xyz = lidar_xyz.to(device)\n",
    "    imb_embs = cilp_model.img_embedder(rgb_img)\n",
    "    # get lidar embeddings from lidar classifier as \"ground truth\"\n",
    "    lidar_embs = lidar_classifier.get_embs(lidar_xyz)\n",
    "    pred_lidar_embs = model(imb_embs)\n",
    "    return pred_lidar_embs, lidar_embs\n",
    "\n",
    "def projector_loss(pred_lidar_embs, lidar_embs):\n",
    "    # simple MSE loss to compare similarity between embeddings\n",
    "    return nn.MSELoss()(pred_lidar_embs, lidar_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e72559",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = training.initWandbRun(\n",
    "    \"\", epochs, BATCH_SIZE, projector_num_params, \"Adam\", \"\", lr, lr\n",
    ")\n",
    "\n",
    "projector_train_loss, projector_val_loss = training.train_model(\n",
    "    projector, \n",
    "    optim,\n",
    "    apply_projector_model, \n",
    "    projector_loss, \n",
    "    epochs, \n",
    "    train_dataloader, \n",
    "    val_dataloader, \n",
    "    device, \n",
    "    run, \n",
    "    scheduler=None, \n",
    "    output_name=\"projector\", \n",
    "    calc_accuracy=False\n",
    ")\n",
    "\n",
    "run.finish()\n",
    "\n",
    "visualization.plot_loss(epochs,\n",
    "    {\n",
    "        \"Projector Train Loss\": projector_train_loss,\n",
    "        \"Projector Val Loss\": projector_val_loss\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70d81be",
   "metadata": {},
   "source": [
    "These are the loss curves I got in my experiment. The minimal validation MSE is 0.0045.\n",
    "\n",
    "![](../results/wandb-t5-projector.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b2e9e5",
   "metadata": {},
   "source": [
    "Again, load the best saved weights for the projector model. This time we don't freeze it as we fine-tune it inside the final RGB-to-Lidar classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fba379",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = models.Projector(source_embedding_dim, target_embedding_dim)\n",
    "projector.load_state_dict(torch.load(\"../checkpoints/projector.pt\", map_location=device))\n",
    "projector = projector.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0335fb69",
   "metadata": {},
   "source": [
    "## Final RGB-to-Lidar Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb291ba",
   "metadata": {},
   "source": [
    "Now we put this pretrained projector in action by constructing a new model to classify images. This `RGB2LidarClassifier` takes raw images, creates image embeddings using CILP, predicts lidar embeddings from it using the projector, and lastly uses the classifier head for a decision. Since the CILP embedder and the classifier head are frozen, only the projector is further fine-tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72489efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgbToLiderClassifier = models.RGB2LiDARClassifier(projector, cilp_model, lidar_classifier)\n",
    "rgbToLidar_num_params = sum(p.numel() for p in rgbToLiderClassifier.parameters())\n",
    "\n",
    "epochs = 50\n",
    "start_lr = 1e-6\n",
    "# we explicitly give the optimizer only the projector parameters so that only the projector is trained\n",
    "optim = torch.optim.Adam(projector.parameters(), lr=lr, weight_decay=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23192af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rgb_Lidar_Classifier_model(model, batch):\n",
    "    inputs_rgb, _, target = batch\n",
    "    inputs_rgb = inputs_rgb.to(device)\n",
    "    target = target.to(device)\n",
    "    outputs = model(inputs_rgb)\n",
    "    return outputs, target\n",
    "\n",
    "loss_func = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0638757a",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = training.initWandbRun(\n",
    "    \"\", epochs, BATCH_SIZE, rgbToLidar_num_params, \"Adam\", \"\", lr, lr\n",
    ")\n",
    "\n",
    "final_train_loss, final_val_loss = training.train_model(\n",
    "    rgbToLiderClassifier, \n",
    "    optim,\n",
    "    apply_rgb_Lidar_Classifier_model, \n",
    "    loss_func, \n",
    "    epochs, \n",
    "    train_dataloader, \n",
    "    val_dataloader, \n",
    "    device, \n",
    "    run, \n",
    "    scheduler=None, \n",
    "    output_name=\"rgb_to_lidar_classifier\"\n",
    ")\n",
    "\n",
    "run.finish()\n",
    "\n",
    "visualization.plot_loss(epochs,\n",
    "    {\n",
    "        \"Rgb-to-Lidar Train Loss\": final_train_loss,\n",
    "        \"Rgb-to-Lidar Val Loss\": final_val_loss\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bf4f63",
   "metadata": {},
   "source": [
    "Unfortunally, even after around 15-20 hours of debugging over multiple days I couldn't get this last model to perform well.\n",
    "The loss curves don't look good. While it seems like training loss is decreasing very slightly, validation loss does not get better at all. The prediction accuracy is just around 0.5 which tells us the model is not any better than just random guessing. \n",
    "\n",
    "![](../results/wandb-t5-final.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f33909",
   "metadata": {},
   "source": [
    "I used the following code to somehow debug and understand the problem. This takes the first validation batch and calculates the classification accuracy when using the \"true\" embeddings using the embedder from the lidar classifier, and compares it this the accuracy using the predicted lidar embeddings using the projector. I also compute cosine similarity of the true and predicted embeddings as well as their norm to understand how similar or different they are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640c087f",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector.eval()\n",
    "with torch.no_grad():\n",
    "    batch = next(iter(val_dataloader))\n",
    "    rgb, lidar_xyza, labels = batch\n",
    "    rgb = rgb.to(device)\n",
    "    lidar_xyza = lidar_xyza.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # True LiDAR path\n",
    "    true_embs = lidar_classifier.get_embs(lidar_xyza)\n",
    "    logits_true = lidar_classifier(data_embs=true_embs).squeeze(1)\n",
    "    acc_true = ((torch.sigmoid(logits_true) >= 0.5) == labels.bool()).float().mean()\n",
    "\n",
    "    # Projected path\n",
    "    img_embs = cilp_model.img_embedder(rgb)\n",
    "    proj_embs = projector(img_embs)\n",
    "    logits_proj = lidar_classifier(data_embs=proj_embs).squeeze(1)\n",
    "    acc_proj = ((torch.sigmoid(logits_proj) >= 0.5) == labels.bool()).float().mean()\n",
    "\n",
    "    # Alignment stats\n",
    "    cosine = torch.nn.functional.cosine_similarity(proj_embs, true_embs, dim=1).mean()\n",
    "    norm_true = true_embs.norm(dim=1).mean()\n",
    "    norm_proj = proj_embs.norm(dim=1).mean()\n",
    "\n",
    "print(f\"Acc true: {acc_true:.3f} | Acc proj: {acc_proj:.3f}\")\n",
    "print(f\"Cosine sim: {cosine:.3f} | Norm true: {norm_true:.3f} | Norm proj: {norm_proj:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661549b2",
   "metadata": {},
   "source": [
    "In my last run I got the following output:\n",
    "\n",
    "```\n",
    "Acc true: 0.969 | Acc proj: 0.625\n",
    "Cosine sim: 0.435 | Norm true: 1.000 | Norm proj: 1.000\n",
    "```\n",
    "\n",
    "The accuracy when using the true lidar embeddings is quite high, which is exptected given that the lidar_classifier has an overall validation accuracy of nearly 1. This indicates that the classification head is OK. The accuracy when using the projected embeddings is at least over 0.5, so not random. This indicates that the projector at least learns *anything*. The norm of both embeddings is exactly 1 which means the enforced normalization in the projector works. The real problem is the small cosine similarity of just 0.435. This tells us that the true and predicted embeddings are not well aligned and off in direction.\n",
    "\n",
    "What I also noticed is that the two-phase training of the projector (first MSE, second BCE inside the RGB2Lidar classifier) is difficult. Often the BCE fine-tuning just tends to destabilize the embedding alignment making the projector's accuracy worse. Thats why I ended up using only a very tiny learning rate for the BCE fine-tuning. \n",
    "\n",
    "Things I tried:\n",
    "- Using Cosine Similarity as the main loss function of the projector pre-training **instead** of MSE (-> did not help)\n",
    "- Mixing MSE with Cosine Similarity in the loss function (-> did not help)\n",
    "- Using different embedding dimensions for image and lidar (-> worked best when both are the same)\n",
    "- not normalizing the embeddings in the lidar classifier as well as in CILP and the projector (-> normalizing was better)\n",
    "- Using a more complex projector architecture (-> extrem overfitting)\n",
    "- Using different learning rates / weight-decay (higher learning rates -> overfitting)\n",
    "- Trying out the exact code used in the NVIDIA lab, which worked there extremely well but did not work here :("
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CompVision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
