{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "235ff225",
   "metadata": {},
   "source": [
    "# Dataset Exploration\n",
    "\n",
    "## Data Preperation\n",
    "\n",
    "From the original dataset ([kaggle](https://www.kaggle.com/datasets/andandand/cubes-and-spheres-lidar-and-rgb)) consisting of 9999 spheres and 9999 cubes each having rgb and lidar data, I selected a subset of 1000 cubes and 1000 spheres. I uploaded this subset as a grouped fiftyone dataset to huggingface [here](https://huggingface.co/datasets/MatthiasCr/multimodal-shapes-subset). In this and all of the following notebooks and experiments I will always start with this huggingface dataset.\n",
    "The fiftyone dataset already has tags for train and validation split to ensure that we will use the same split for all experiments. The azimuth and zenith data for the lidar samples is added as dataset-level metadata fields. \n",
    "\n",
    "The following is the code I used to create this subset and push it to huggingface. The code loads the original full dataset from google drive, creates the grouped fiftyone dataset, adds the azimuth and zenith data, and creates a train / val split. \n",
    "For this notebook this code must not be executed, I just added it for completeness.\n",
    "\n",
    "```python\n",
    "from google.colab import drive\n",
    "import random\n",
    "import fiftyone as fo\n",
    "import fiftyone.utils.random as four\n",
    "from fiftyone.utils.huggingface import push_to_hub\n",
    "import numpy as np\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "data_root = \"/content/drive/MyDrive/compVision/spheres_and_cubes\"\n",
    "\n",
    "# randomly choose 1000 out of the 9999 indices of the original dataset\n",
    "N = 9999\n",
    "indices = sorted(random.sample(range(N), int(0.1 * N) + 1))\n",
    "\n",
    "# create a gouped fiftyone dataset\n",
    "dataset = fo.Dataset(\"multimodal_shapes_subset\")\n",
    "dataset.add_group_field(\"group\")\n",
    "\n",
    "shapes = {\n",
    "    \"cubes\": \"cube\",\n",
    "    \"spheres\": \"sphere\",\n",
    "}\n",
    "\n",
    "# using the chosen indices add 1000 sphere and 1000 cube samples to fiftyone dataset\n",
    "groups = {}\n",
    "for cls, label in shapes.items():\n",
    "    for i in indices:\n",
    "        gid = f\"{cls}_{i:04d}\"\n",
    "\n",
    "        groups.setdefault(gid, fo.Group())\n",
    "\n",
    "        # rgb sample\n",
    "        rgb = fo.Sample(\n",
    "            filepath=f\"{data_root}/{cls}/rgb/{i:04d}.png\",\n",
    "            group = groups[gid].element(\"rgb\"),\n",
    "            label=fo.Classification(label=label),\n",
    "        )\n",
    "\n",
    "        # lidar sample\n",
    "        lidar = fo.Sample(\n",
    "            filepath=f\"{data_root}/{cls}/lidar/{i:04d}.npy\",\n",
    "            group = groups[gid].element(\"lidar\"),\n",
    "            label=fo.Classification(label=label),\n",
    "        )\n",
    "        dataset.add_samples([rgb, lidar])\n",
    "\n",
    "# also load azimuth and zenith data and them it as dataset metadata fields\n",
    "azimuth_cubes = np.load(f\"{data_root}/cubes/azimuth.npy\")\n",
    "zenith_cubes  = np.load(f\"{data_root}/cubes/zenith.npy\")\n",
    "azimuth_spheres = np.load(f\"{data_root}/spheres/azimuth.npy\")\n",
    "zenith_spheres = np.load(f\"{data_root}/spheres/zenith.npy\")\n",
    "\n",
    "dataset.info[\"azimuth_cubes\"] = azimuth_cubes.tolist()\n",
    "dataset.info[\"zenith_cubes\"] = zenith_cubes.tolist()\n",
    "dataset.info[\"azimuth_spheres\"] = azimuth_spheres.tolist()\n",
    "dataset.info[\"zenith_spheres\"] = zenith_spheres.tolist()\n",
    "dataset.save()\n",
    "\n",
    "# train / val split per class so the classes remain balanced inside train and val split\n",
    "spheres = dataset.match(F(\"label.label\") == \"sphere\")\n",
    "four.random_split(spheres, {\"train\": 0.8, \"val\": 0.2})\n",
    "cubes = dataset.match(F(\"label.label\") == \"cube\")\n",
    "four.random_split(cubes, {\"train\": 0.8, \"val\": 0.2})\n",
    "\n",
    "# push fiftyone dataset to huggingface\n",
    "push_to_hub(dataset, \"multimodal-shapes-subset\", label_field=\"label\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aafb880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Colab-only setup\n",
    "if \"google.colab\" in sys.modules:\n",
    "    print(\"Running in Google Colab. Setting up repo\")\n",
    "\n",
    "    !git clone https://github.com/MatthiasCr/Computer-Vision-Assignment-2.git\n",
    "    %cd Computer-Vision-Assignment-2\n",
    "    !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241e120b",
   "metadata": {},
   "source": [
    "## Load the Data and Visualize in Fiftyone\n",
    "\n",
    "Now we can really start by loading the dataset from huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0d52dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import fiftyone as fo\n",
    "from fiftyone import ViewField as F\n",
    "import fiftyone.core.groups as fog\n",
    "from fiftyone.utils.huggingface import load_from_hub\n",
    "import numpy as np\n",
    "import open3d as o3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a497fe74",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_hub(\"MatthiasCr/multimodal-shapes-subset\", \n",
    "    name=\"multimodal-shapes-subset\",\n",
    "    # fewer workers and greater batch size to hopefully avoid getting rate limited\n",
    "    num_workers=2,\n",
    "    batch_size=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfda00fd",
   "metadata": {},
   "source": [
    "Fiftyone can't visualize lidar data stored in raw npy files. To really visualize the lidar data in fiftyone, we first convert the lidar depth to xyz coordinates using the azimuth and zenith data. Then we use open3d to convert it to point clouds and store it as pcd files. These pcd files we then add as a third slice to the grouped dataset, so that every group has 3 samples: rgb, lidar (npy), and pcd. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48585e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load azimuth and zenith data from dataset-level metadata fields\n",
    "azimuth_cubes = np.array(dataset.info[\"azimuth_cubes\"])\n",
    "zenith_cubes  = np.array(dataset.info[\"zenith_cubes\"])\n",
    "azimuth_spheres = np.array(dataset.info[\"azimuth_spheres\"])\n",
    "zenith_spheres = np.array(dataset.info[\"zenith_spheres\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e303599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert lidar depth to xyza given azimuth and zenith data\n",
    "def get_xyza(lidar_depth, azimuth, zenith):\n",
    "    x = lidar_depth * np.sin(-azimuth[:, None]) * np.cos(-zenith[None, :])\n",
    "    y = lidar_depth * np.cos(-azimuth[:, None]) * np.cos(-zenith[None, :])\n",
    "    z = lidar_depth * np.sin(-zenith[None, :])\n",
    "    a = np.where(lidar_depth < 50.0, np.ones_like(lidar_depth), np.zeros_like(lidar_depth))\n",
    "    xyza = np.stack((x, y, z, a))\n",
    "    return xyza"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa301dde",
   "metadata": {},
   "source": [
    "Now for each lidar sample we create a point cloud pcd file and add it as a third sample to the group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d23f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local directory for .pcd files\n",
    "os.makedirs(\"./data\", exist_ok=True)\n",
    "\n",
    "lidar_view = dataset.select_group_slices(\"lidar\")\n",
    "\n",
    "# iterate over all lidar samples\n",
    "for idx, sample in enumerate(lidar_view):\n",
    "    lidar_path = sample.filepath\n",
    "    lidar_depth = np.load(lidar_path)\n",
    "\n",
    "    if sample.label.label == \"cube\":\n",
    "        az = azimuth_cubes\n",
    "        ze = zenith_cubes\n",
    "    else:\n",
    "        az = azimuth_spheres\n",
    "        ze = zenith_spheres\n",
    "\n",
    "    xyza_data = get_xyza(lidar_depth, az, ze)\n",
    "    xyz = xyza_data[:3].reshape(3, -1).T\n",
    "    a = xyza_data[3].reshape(-1).astype(bool)\n",
    "    # apply mask\n",
    "    xyz_valid = xyz[a]\n",
    "\n",
    "    # create point cloud\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(xyz_valid)\n",
    "    pcd_filepath = f\"./data/{idx}.pcd\"\n",
    "    o3d.io.write_point_cloud(pcd_filepath, pcd)\n",
    "\n",
    "    # add pcd as third sample to the group\n",
    "    pcd_sample = fo.Sample(\n",
    "        filepath=pcd_filepath,\n",
    "        label=sample.label,\n",
    "    )\n",
    "\n",
    "    # Attach to same group under a new slice\n",
    "    pcd_sample['group'] = sample.group.element(\"pcd\")\n",
    "\n",
    "    dataset.add_sample(pcd_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89130cc8",
   "metadata": {},
   "source": [
    "Launch the app to visualize the point clouds and explore the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e875a400",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = fo.launch_app(dataset=dataset, auto=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74272208",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(session.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c60985",
   "metadata": {},
   "source": [
    "![](../results/fo-rgb-slices.png)\n",
    "![](../results/fo-cube-rgb-and-pcd.png)\n",
    "![](../results/fo-sphere-rgb-and-pcd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a42dff4",
   "metadata": {},
   "source": [
    "## Dataset Statistics and Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60559ec6",
   "metadata": {},
   "source": [
    "We first inspect datatypes and size of the images and lidar data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0814a600",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_view = dataset.select_group_slices(\"rgb\")\n",
    "lidar_view = dataset.select_group_slices(\"lidar\")\n",
    "\n",
    "\n",
    "image_size = Image.open(rgb_view.first().filepath).size\n",
    "lidar_dtype = np.load(lidar_view.first().filepath).dtype\n",
    "lidar_size = np.load(lidar_view.first().filepath).shape\n",
    "\n",
    "print(f\"Image size: {image_size}\")\n",
    "print(f\"Lidar dtype: {lidar_dtype}\")\n",
    "print(f\"Lidar size: {lidar_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057653f8",
   "metadata": {},
   "source": [
    "Output:\n",
    "\n",
    "```\n",
    "Image size: (64, 64)\n",
    "Lidar dtype: float32\n",
    "Lidar size: (64, 64)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb99add",
   "metadata": {},
   "source": [
    "Now we calculate number of samples per class, train/val split size, and class distribution in train split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef40e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sphere_view = dataset.match(F(\"label.label\") == \"sphere\")\n",
    "cube_view = dataset.match(F(\"label.label\") == \"cube\")\n",
    "\n",
    "train_view = rgb_view.match_tags(\"train\")\n",
    "train_percentage = (int)((train_view.count() / rgb_view.count()) * 100)\n",
    "sphere_train_view = train_view.match(F(\"label.label\") == \"sphere\")\n",
    "cube_train_view = train_view.match(F(\"label.label\") == \"cube\")\n",
    "\n",
    "print(\"Class distribution:\")\n",
    "print(f\"Overall number of samples: {dataset.count()}\")\n",
    "print(f\"spheres: {sphere_view.count()}\")\n",
    "print(f\"cubes: {cube_view.count()}\")\n",
    "print()\n",
    "print(\"Train/Val Split:\")\n",
    "print(f\"Overall number of train samples: {train_view.count()} ({train_percentage}%)\")\n",
    "print(f\"train spheres: {sphere_train_view.count()}\")\n",
    "print(f\"train cubes: {cube_train_view.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59191656",
   "metadata": {},
   "source": [
    "Output:\n",
    "\n",
    "```\n",
    "Class distribution:\n",
    "Overall number of samples: 2000\n",
    "spheres: 1000\n",
    "cubes: 1000\n",
    "\n",
    "Train/Val Split:\n",
    "Overall number of train samples: 1600 (80%)\n",
    "train spheres: 800\n",
    "train cubes: 800\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7d677e",
   "metadata": {},
   "source": [
    "Overall these are the dataset statistics:\n",
    "\n",
    "- Total samples per class: 1000 cubes, 1000 spheres (overall 2000 samples; 6000 including the 3-slice groups)\n",
    "- Train/val split: 1600 train (800 per class), 400 val (200 per class)\n",
    "- The classes are perfectly balanced\n",
    "- Image size/dtypes: RGB 64×64 uint8, LiDAR 64×64 float32\n",
    "\n",
    "Observations after visual inspection:\n",
    "\n",
    "- Objects are not centered at 0/0/0 because this is probably where the simulated LiDAR sensor lives. Objects are in front of that sensor\n",
    "- Objects have a single color and the images have a black background\n",
    "- Images as well as LiDAR data is low-res (64×64) which could limit the detection of details but should be OK for such simple shapes\n",
    "- We need to crop LiDAR points beyond 50m as this is the sensors maximum reach. For this we use the a mask in `get_xyza`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34065a0a",
   "metadata": {},
   "source": [
    "We can also plot histograms using fiftyone:\n",
    "\n",
    "![](../results/fo-hist-classdist.png)\n",
    "![](../results/fo-hist-trainval.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compvisiontest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
